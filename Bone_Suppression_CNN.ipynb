{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Logic06183/AB-testing/blob/main/Bone_Suppression_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "desperate-machine"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv).\n",
        "import random\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Input, MaxPooling2D, concatenate, Conv2DTranspose\n",
        "import tqdm\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "id": "desperate-machine"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Sd1_u6Zier6",
        "outputId": "7173d9ec-2fce-4e9f-8f5e-ed95386b6f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "1Sd1_u6Zier6"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "quick-democracy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b914b2b-8877-47fb-e202-85452db63ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "#since the dataset isn't organized, we can't rely on existing keras datagenerator objects. Getting the filenames lets us match input/output for batch generation\n",
        "output_files = []\n",
        "input_files = []\n",
        "files=[]\n",
        "for dirname, _, filenames in os.walk('../content/drive/My Drive/Colab Notebooks/augmented/source/'):\n",
        "    for filename in filenames:\n",
        "        #input_files.append(os.path.join(dirname, filename))\n",
        "        files.append(filename)\n",
        "\n",
        "\n",
        "num_files = len(files)\n",
        "print(num_files)\n",
        "\n",
        "#split files into training, validation, testing (without bothering to restructure file structure)\n",
        "\n",
        "random.shuffle(files)\n",
        "\n",
        "training_files = files[:int(num_files*.8)]\n",
        "validation_files = files[int(num_files*.8):int(num_files*.9)]\n",
        "test_files = files[int(num_files*.9):]\n",
        "\n",
        "\n",
        "\n",
        "print(len(training_files) + len(validation_files) + len(test_files)) #should be the same as num_files\n",
        "\n"
      ],
      "id": "quick-democracy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piano-following"
      },
      "outputs": [],
      "source": [
        "def dataGrab(input_path,output_path,files):\n",
        "    input = []\n",
        "    for file in tqdm.tqdm(files):\n",
        "        im = Image.open(os.path.join(input_path,file))\n",
        "        im = im.resize((256,256)) #needed to stop running out of RAM\n",
        "        data = np.asarray(im).astype('float32')/255.0\n",
        "        input.append(data)\n",
        "\n",
        "\n",
        "    output = []\n",
        "    for file in tqdm.tqdm(files):\n",
        "        im = Image.open(os.path.join(output_path,file))\n",
        "        im = im.resize((256,256))\n",
        "        data = np.asarray(im).astype('float32')/255.0\n",
        "        output.append(data)\n",
        "\n",
        "    return input, output\n",
        "\n",
        "\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self,files,input_path,output_path,batch_size = 32):\n",
        "        self.files = files\n",
        "        self.batch_size = batch_size\n",
        "        #self.dim = dim\n",
        "        self.input_path = input_path\n",
        "        self.output_path  = output_path\n",
        "        self.indexes = np.arange(len(self.files))\n",
        "\n",
        "        #Here is where we load in all the data\n",
        "        self.input_data,self.output_data = dataGrab(self.input_path,self.output_path,self.files)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        #Gives number of batches per epoch\n",
        "        return int(np.floor(len(self.files)/self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        X = np.zeros((self.batch_size,256,256,1))\n",
        "        y = np.zeros((self.batch_size,256,256,1))\n",
        "\n",
        "        for i, ID in enumerate(indexes):\n",
        "            X[i,:,:,0] = self.input_data[ID]\n",
        "            y[i,:,:,0] = self.output_data[ID]\n",
        "\n",
        "        return X,y\n",
        "\n"
      ],
      "id": "piano-following"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rotary-protein"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_generator = DataGenerator(training_files,\n",
        "                             input_path = '../content/drive/My Drive/Colab Notebooks/augmented/source/',\n",
        "                             output_path = '../content/drive/My Drive/Colab Notebooks/augmented/target/')\n",
        "\n",
        "val_generator = DataGenerator(validation_files,\n",
        "                             input_path = '../content/drive/My Drive/Colab Notebooks/augmented/source/',\n",
        "                             output_path = '../content/drive/My Drive/Colab Notebooks/augmented/target/')"
      ],
      "id": "rotary-protein"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thousand-bouquet"
      },
      "source": [
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    Input(shape=(256,256,1)),\n",
        "    Conv2D(64, kernel_size=(7,7), activation='elu',padding=\"same\"),\n",
        "    Conv2D(32, kernel_size=(5,5), activation='elu',padding=\"same\"),\n",
        "    Conv2D(32, kernel_size=(5,5), activation='elu',padding=\"same\"),\n",
        "    Conv2D(32, kernel_size=(3,3), activation='elu',padding=\"same\"),\n",
        "    Conv2D(32, kernel_size=(3,3), activation='elu',padding=\"same\"),\n",
        "    Conv2D(1, kernel_size=3, activation='relu',padding=\"same\"),  \n",
        "])\n",
        "filepath = \"epoch-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "model.compile(optimizer =\"adam\",loss=\"mse\",metrics = [\"Accuracy\"])\n",
        "model.summary()"
      ],
      "id": "thousand-bouquet"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "discrete-senegal",
        "outputId": "cf1c2756-a0b8-47b9-aa20-bab1876d6f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 256, 256, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 256, 256, 32  320         ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 256, 256, 32  9248        ['conv2d[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 128, 128, 32  0           ['conv2d_1[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 128, 128, 64  18496       ['max_pooling2d[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 128, 128, 64  36928       ['conv2d_2[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)  0           ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 64, 64, 128)  73856       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 64, 64, 128)  147584      ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0          ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 256)  295168      ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 256)  590080      ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0          ['conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 16, 16, 512)  1180160     ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 16, 16, 512)  2359808     ['conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTransp  (None, 32, 32, 256)  524544     ['conv2d_9[0][0]']               \n",
            " ose)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32, 32, 512)  0           ['conv2d_transpose[0][0]',       \n",
            "                                                                  'conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 256)  1179904     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 256)  590080      ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 128)  131200     ['conv2d_11[0][0]']              \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 64, 64, 256)  0           ['conv2d_transpose_1[0][0]',     \n",
            "                                                                  'conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 64, 64, 128)  295040      ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 64, 64, 128)  147584      ['conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 64  32832      ['conv2d_13[0][0]']              \n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 128, 128, 12  0           ['conv2d_transpose_2[0][0]',     \n",
            "                                8)                                'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 128, 128, 64  73792       ['concatenate_2[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 128, 128, 64  36928       ['conv2d_14[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2DTran  (None, 256, 256, 32  8224       ['conv2d_15[0][0]']              \n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 256, 256, 64  0           ['conv2d_transpose_3[0][0]',     \n",
            "                                )                                 'conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 256, 256, 32  18464       ['concatenate_3[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 256, 256, 32  9248        ['conv2d_16[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 256, 256, 1)  33          ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,759,521\n",
            "Trainable params: 7,759,521\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#No idea to create a U-Net, but let's try\n",
        "#borrowed from https://www.kaggle.com/eduardomineo/u-net-lung-segmentation-montgomery-shenzhen#3.-Segmentation-training\n",
        "def unet(input_size=(256,256,1)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "\n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
        "\n",
        "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs], outputs=[conv10])\n",
        "\n",
        "model = unet()\n",
        "filepath = \"epoch-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, mode='min')\n",
        "model.compile(optimizer =\"adam\",loss=\"mse\")\n",
        "model.summary()"
      ],
      "id": "discrete-senegal"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naughty-static"
      },
      "outputs": [],
      "source": [
        "model.fit(training_generator,validation_data=val_generator,epochs=20,callbacks = [checkpoint,tf.keras.callbacks.TensorBoard(\"logs\"),tf.keras.callbacks.CSVLogger(\"training_loss.csv\")])\n",
        "#model.fit(val_generator,epochs=3)"
      ],
      "id": "naughty-static"
    },
    {
      "cell_type": "code",
      "source": [
        "# convert testing set to numpy array to fit in memory (don't do that when testing\n",
        "# set is too large)\n",
        "y_test = np.zeros((n_testing_samples,))\n",
        "X_test = np.zeros((n_testing_samples, 299, 299, 3))\n",
        "for i, (img, label) in enumerate(test_ds.take(n_testing_samples)):\n",
        "  # print(img.shape, label.shape)\n",
        "  X_test[i] = img\n",
        "  y_test[i] = label.numpy()\n",
        "\n",
        "print(\"y_test.shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "odch3HpVoRrn"
      },
      "id": "odch3HpVoRrn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-g3mQJ7vma4"
      },
      "outputs": [],
      "source": [
        "# load the weights with the least loss\n",
        "model.load_weights(\"benign-vs-malignant_64_rmsprop_0.410.h5\")"
      ],
      "id": "W-g3mQJ7vma4"
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = m.evaluate(X_test, y_test, verbose=0)"
      ],
      "metadata": {
        "id": "_apExYwbnrhZ"
      },
      "id": "_apExYwbnrhZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "img = cv2.imread('/kaggle/input/padchest-chest-xrays-sample/sample/216840111366964012819207061112010316094555679_04-017-068.png', 0)\n",
        "pred = model.predict(img)"
      ],
      "metadata": {
        "id": "pjQHZx5oVzVL"
      },
      "id": "pjQHZx5oVzVL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6695.250063,
      "end_time": "2021-06-24T23:13:25.077394",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-06-24T21:21:49.827331",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}